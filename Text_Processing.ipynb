{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eMcjXTTmU0nR",
        "outputId": "24fd6601-1919-42d2-9160-73452dfab71c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting contractions\n",
            "  Downloading contractions-0.1.73-py2.py3-none-any.whl (8.7 kB)\n",
            "Collecting textsearch>=0.0.21 (from contractions)\n",
            "  Downloading textsearch-0.0.24-py2.py3-none-any.whl (7.6 kB)\n",
            "Collecting anyascii (from textsearch>=0.0.21->contractions)\n",
            "  Downloading anyascii-0.3.2-py3-none-any.whl (289 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m289.9/289.9 kB\u001b[0m \u001b[31m5.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting pyahocorasick (from textsearch>=0.0.21->contractions)\n",
            "  Downloading pyahocorasick-2.0.0-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_12_x86_64.manylinux2010_x86_64.whl (110 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m110.8/110.8 kB\u001b[0m \u001b[31m12.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: pyahocorasick, anyascii, textsearch, contractions\n",
            "Successfully installed anyascii-0.3.2 contractions-0.1.73 pyahocorasick-2.0.0 textsearch-0.0.24\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting Unidecode\n",
            "  Downloading Unidecode-1.3.6-py3-none-any.whl (235 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m235.9/235.9 kB\u001b[0m \u001b[31m5.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: Unidecode\n",
            "Successfully installed Unidecode-1.3.6\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting Pattern\n",
            "  Downloading Pattern-3.6.0.tar.gz (22.2 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m22.2/22.2 MB\u001b[0m \u001b[31m48.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: future in /usr/local/lib/python3.10/dist-packages (from Pattern) (0.18.3)\n",
            "Collecting backports.csv (from Pattern)\n",
            "  Downloading backports.csv-1.0.7-py2.py3-none-any.whl (12 kB)\n",
            "Collecting mysqlclient (from Pattern)\n",
            "  Downloading mysqlclient-2.1.1.tar.gz (88 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m88.1/88.1 kB\u001b[0m \u001b[31m9.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: beautifulsoup4 in /usr/local/lib/python3.10/dist-packages (from Pattern) (4.11.2)\n",
            "Requirement already satisfied: lxml in /usr/local/lib/python3.10/dist-packages (from Pattern) (4.9.2)\n",
            "Collecting feedparser (from Pattern)\n",
            "  Downloading feedparser-6.0.10-py3-none-any.whl (81 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m81.1/81.1 kB\u001b[0m \u001b[31m8.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting pdfminer.six (from Pattern)\n",
            "  Downloading pdfminer.six-20221105-py3-none-any.whl (5.6 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.6/5.6 MB\u001b[0m \u001b[31m68.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from Pattern) (1.22.4)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.10/dist-packages (from Pattern) (1.10.1)\n",
            "Requirement already satisfied: nltk in /usr/local/lib/python3.10/dist-packages (from Pattern) (3.8.1)\n",
            "Collecting python-docx (from Pattern)\n",
            "  Downloading python-docx-0.8.11.tar.gz (5.6 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.6/5.6 MB\u001b[0m \u001b[31m98.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting cherrypy (from Pattern)\n",
            "  Downloading CherryPy-18.8.0-py2.py3-none-any.whl (348 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m348.4/348.4 kB\u001b[0m \u001b[31m34.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from Pattern) (2.27.1)\n",
            "Requirement already satisfied: soupsieve>1.2 in /usr/local/lib/python3.10/dist-packages (from beautifulsoup4->Pattern) (2.4.1)\n",
            "Collecting cheroot>=8.2.1 (from cherrypy->Pattern)\n",
            "  Downloading cheroot-9.0.0-py2.py3-none-any.whl (100 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m100.6/100.6 kB\u001b[0m \u001b[31m12.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting portend>=2.1.1 (from cherrypy->Pattern)\n",
            "  Downloading portend-3.1.0-py3-none-any.whl (5.3 kB)\n",
            "Requirement already satisfied: more-itertools in /usr/local/lib/python3.10/dist-packages (from cherrypy->Pattern) (9.1.0)\n",
            "Collecting zc.lockfile (from cherrypy->Pattern)\n",
            "  Downloading zc.lockfile-3.0.post1-py3-none-any.whl (9.8 kB)\n",
            "Collecting jaraco.collections (from cherrypy->Pattern)\n",
            "  Downloading jaraco.collections-4.1.0-py3-none-any.whl (11 kB)\n",
            "Collecting sgmllib3k (from feedparser->Pattern)\n",
            "  Downloading sgmllib3k-1.0.0.tar.gz (5.8 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.10/dist-packages (from nltk->Pattern) (8.1.3)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.10/dist-packages (from nltk->Pattern) (1.2.0)\n",
            "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.10/dist-packages (from nltk->Pattern) (2022.10.31)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from nltk->Pattern) (4.65.0)\n",
            "Requirement already satisfied: charset-normalizer>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from pdfminer.six->Pattern) (2.0.12)\n",
            "Requirement already satisfied: cryptography>=36.0.0 in /usr/local/lib/python3.10/dist-packages (from pdfminer.six->Pattern) (40.0.2)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->Pattern) (1.26.15)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->Pattern) (2022.12.7)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->Pattern) (3.4)\n",
            "Requirement already satisfied: six>=1.11.0 in /usr/local/lib/python3.10/dist-packages (from cheroot>=8.2.1->cherrypy->Pattern) (1.16.0)\n",
            "Collecting jaraco.functools (from cheroot>=8.2.1->cherrypy->Pattern)\n",
            "  Downloading jaraco.functools-3.6.0-py3-none-any.whl (7.9 kB)\n",
            "Requirement already satisfied: cffi>=1.12 in /usr/local/lib/python3.10/dist-packages (from cryptography>=36.0.0->pdfminer.six->Pattern) (1.15.1)\n",
            "Collecting tempora>=1.8 (from portend>=2.1.1->cherrypy->Pattern)\n",
            "  Downloading tempora-5.2.2-py3-none-any.whl (13 kB)\n",
            "Collecting jaraco.text (from jaraco.collections->cherrypy->Pattern)\n",
            "  Downloading jaraco.text-3.11.1-py3-none-any.whl (11 kB)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from zc.lockfile->cherrypy->Pattern) (67.7.2)\n",
            "Requirement already satisfied: pycparser in /usr/local/lib/python3.10/dist-packages (from cffi>=1.12->cryptography>=36.0.0->pdfminer.six->Pattern) (2.21)\n",
            "Requirement already satisfied: pytz in /usr/local/lib/python3.10/dist-packages (from tempora>=1.8->portend>=2.1.1->cherrypy->Pattern) (2022.7.1)\n",
            "Collecting jaraco.context>=4.1 (from jaraco.text->jaraco.collections->cherrypy->Pattern)\n",
            "  Downloading jaraco.context-4.3.0-py3-none-any.whl (5.3 kB)\n",
            "Collecting autocommand (from jaraco.text->jaraco.collections->cherrypy->Pattern)\n",
            "  Downloading autocommand-2.2.2-py3-none-any.whl (19 kB)\n",
            "Requirement already satisfied: inflect in /usr/local/lib/python3.10/dist-packages (from jaraco.text->jaraco.collections->cherrypy->Pattern) (6.0.4)\n",
            "Requirement already satisfied: pydantic>=1.9.1 in /usr/local/lib/python3.10/dist-packages (from inflect->jaraco.text->jaraco.collections->cherrypy->Pattern) (1.10.7)\n",
            "Requirement already satisfied: typing-extensions>=4.2.0 in /usr/local/lib/python3.10/dist-packages (from pydantic>=1.9.1->inflect->jaraco.text->jaraco.collections->cherrypy->Pattern) (4.5.0)\n",
            "Building wheels for collected packages: Pattern, mysqlclient, python-docx, sgmllib3k\n",
            "  Building wheel for Pattern (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for Pattern: filename=Pattern-3.6-py3-none-any.whl size=22332702 sha256=d43e9f4d4a98b4712bdb95930e5bb7b896ccd773444e8afc5be1085763a78b63\n",
            "  Stored in directory: /root/.cache/pip/wheels/d1/8f/40/fe23abd593ef60be5bfaf3e02154d3484df42aa947bbf4d499\n",
            "  Building wheel for mysqlclient (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for mysqlclient: filename=mysqlclient-2.1.1-cp310-cp310-linux_x86_64.whl size=108353 sha256=262502000fc184328a47aac413edd50bcb4833c368f65365f9b0c792118eb73c\n",
            "  Stored in directory: /root/.cache/pip/wheels/7d/34/ba/a769c165b01646816afdf9bf792e847ef149693fee432b6b65\n",
            "  Building wheel for python-docx (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for python-docx: filename=python_docx-0.8.11-py3-none-any.whl size=184491 sha256=05695c7cce4aaa02cc47945fd71d7538443bf78c9c7064c45982c10e3ecbd150\n",
            "  Stored in directory: /root/.cache/pip/wheels/80/27/06/837436d4c3bd989b957a91679966f207bfd71d358d63a8194d\n",
            "  Building wheel for sgmllib3k (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for sgmllib3k: filename=sgmllib3k-1.0.0-py3-none-any.whl size=6046 sha256=557433a80cabb5a514e4e44a9f05cae6932f78b1518b0570af814a44f0315cbd\n",
            "  Stored in directory: /root/.cache/pip/wheels/f0/69/93/a47e9d621be168e9e33c7ce60524393c0b92ae83cf6c6e89c5\n",
            "Successfully built Pattern mysqlclient python-docx sgmllib3k\n",
            "Installing collected packages: sgmllib3k, backports.csv, zc.lockfile, python-docx, mysqlclient, jaraco.functools, jaraco.context, feedparser, autocommand, tempora, cheroot, portend, pdfminer.six, jaraco.text, jaraco.collections, cherrypy, Pattern\n",
            "Successfully installed Pattern-3.6 autocommand-2.2.2 backports.csv-1.0.7 cheroot-9.0.0 cherrypy-18.8.0 feedparser-6.0.10 jaraco.collections-4.1.0 jaraco.context-4.3.0 jaraco.functools-3.6.0 jaraco.text-3.11.1 mysqlclient-2.1.1 pdfminer.six-20221105 portend-3.1.0 python-docx-0.8.11 sgmllib3k-1.0.0 tempora-5.2.2 zc.lockfile-3.0.post1\n"
          ]
        }
      ],
      "source": [
        "!pip install contractions\n",
        "!pip install Unidecode\n",
        "!pip install Pattern"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2je8cLGb01X2"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import re \n",
        "import unidecode\n",
        "import re \n",
        "import time\n",
        "import string\n",
        "import statistics\n",
        "\n",
        "from datetime import datetime\n",
        "\n",
        "import contractions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JcAY77VLSZDO"
      },
      "outputs": [],
      "source": [
        "def sampleEqually(data_name, sample=0.1, lines=False, text_cleaner=None):\n",
        "  data = pd.read_json(\"drive/MyDrive/\" + data_name, lines = lines)\n",
        "  if(text_cleaner is not None):\n",
        "    data[\"review_text\"] = data[\"review_text\"].apply(text_cleaner)\n",
        "  ## Let's calculate 10% of the data \n",
        "  TOTAL = len(data)\n",
        "  N_SAMPLES = int(TOTAL * sample)\n",
        "  print(\"Total:\", TOTAL)\n",
        "  print(N_SAMPLES)\n",
        "\n",
        "  ## Calcualte number of false and true samples \n",
        "  false_samples = data[data[\"is_spoiler\"] == False]\n",
        "  true_samples = data[data[\"is_spoiler\"] == True]\n",
        "\n",
        "  N_TRUE = len(true_samples)\n",
        "  N_FALSE = len(false_samples)\n",
        "  print(\"No of false:\", N_FALSE)\n",
        "  print(\"No of true:\", N_TRUE)\n",
        "  print(\"% True\", N_TRUE/ TOTAL * 100)\n",
        "  print(\"% False\", N_FALSE/ TOTAL * 100)\n",
        "\n",
        "  ## Get half true and half false \n",
        "\n",
        "  n_false_samples = false_samples.sample(int(N_SAMPLES / 2))\n",
        "  n_true_samples = true_samples.sample(int(N_SAMPLES / 2) +1 )\n",
        "\n",
        "  N_FALSE_FILTERED = len(n_false_samples)\n",
        "  N_TRUE_FILTERED = len(n_true_samples)\n",
        "  print(\"No of false:\", N_FALSE_FILTERED)\n",
        "  print(\"No of true:\", N_TRUE_FILTERED)\n",
        "  print(\"% True\", N_TRUE_FILTERED/ TOTAL * 100)\n",
        "  print(\"% False\", N_FALSE_FILTERED/ TOTAL * 100)\n",
        "\n",
        "  FILTERED_TOTAL = N_TRUE_FILTERED + N_FALSE_FILTERED\n",
        "  ## Selected 10% \n",
        "  print(FILTERED_TOTAL)\n",
        "  print(N_SAMPLES)\n",
        "\n",
        "  filtered_data = pd.concat([n_false_samples, n_true_samples], axis=0)\n",
        "\n",
        "  filtered_data = filtered_data.sample(frac=1)\n",
        "\n",
        "  return filtered_data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8cm6f8b7TNHC"
      },
      "source": [
        "## Cleaning functions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RfPk_uU7Uxri",
        "outputId": "7937f832-c3c9-4c9d-df3a-4163e738b93d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
          ]
        }
      ],
      "source": [
        "import nltk \n",
        "from nltk.corpus import stopwords # Stopwords \n",
        "from nltk.stem.porter import PorterStemmer\n",
        "from pattern.text.en import singularize\n",
        "nltk.download('punkt')\n",
        "from nltk.tokenize import sent_tokenize, word_tokenize\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nBiL2DihU0Le",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2669f334-9c5e-466c-855e-9253eee23835"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 4
        }
      ],
      "source": [
        "nltk.download('stopwords')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bX52Ot_R0cfE"
      },
      "outputs": [],
      "source": [
        "def remove_links(text):\n",
        "    \"\"\"\n",
        "    This function will remove links from the \n",
        "    text contained within the Dataset.\n",
        "       \n",
        "    arguments:\n",
        "        input_text: \"text\" of type \"String\". \n",
        "                    \n",
        "    return:\n",
        "        value: \"text\" with removed links. \n",
        "    \"\"\"\n",
        "    \n",
        "    text = re.sub(r'http\\S+', '', text)\n",
        "    return text"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HxI-M0NeIOqd"
      },
      "outputs": [],
      "source": [
        "def remove_special_characters(text):\n",
        "  text = re.sub(r'[^\\w\\s]', '', text)\n",
        "\n",
        "  return text"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6viIW7CkIQzc"
      },
      "outputs": [],
      "source": [
        "## https://www.geeksforgeeks.org/removing-stop-words-nltk-python/\n",
        "## https://medium.com/analytics-vidhya/removing-stop-words-with-nltk-library-in-python-f33f53556cc1\n",
        "\n",
        "def remove_stopwords(text):\n",
        "  tokens = word_tokenize(text)\n",
        "  english_stopwords = stop_words = set(stopwords.words('english'))\n",
        "  tokens_wo_stopwords = [t for t in tokens if t not in english_stopwords]\n",
        "  text = \" \".join(tokens_wo_stopwords)\n",
        "\n",
        "  return text"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aDEFbwCX0zZS"
      },
      "outputs": [],
      "source": [
        "def remove_double_spaces(text):\n",
        "  text = re.sub(r'\\s{2,}', ' ', text)\n",
        "\n",
        "  return text"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EMptunVfUnJT"
      },
      "outputs": [],
      "source": [
        "# Expand contraction words\n",
        "def expand_contractions(text):\n",
        "    \"\"\"\n",
        "    This Function will expands contractions in words.\n",
        "    \n",
        "    arguments:\n",
        "         input_text: \"text\" of type \"String\".\n",
        "         \n",
        "    return:\n",
        "        value: \"text\" with expanded contractions .  \n",
        "    \"\"\"\n",
        "    \n",
        "    # creating an empty list\n",
        "    expanded_words = []    \n",
        "    for word in text.split():\n",
        "      # using contractions.fix to expand the shotened words\n",
        "      expanded_words.append(contractions.fix(word)) \n",
        "    \n",
        "    String_Of_tokens = ' '.join(expanded_words)\n",
        "    \n",
        "    return String_Of_tokens"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "53KeBKqEUn7i"
      },
      "outputs": [],
      "source": [
        "# Remove accented characters\n",
        "def accented_characters_removal(text):\n",
        "    \"\"\"\n",
        "    This function will remove accented characters from the \n",
        "    text contained within the Dataset.\n",
        "       \n",
        "    arguments:\n",
        "    \n",
        "        input_text: \"text\" of type \"String\". \n",
        "                    \n",
        "    return:\n",
        "        value: \"text\" with removed accented characters.  \n",
        "    \"\"\"\n",
        "    \n",
        "    # Unidecode() - It takes unicode data & tries to represent it to ASCII characters. \n",
        "    text = unidecode.unidecode(text) \n",
        "    \n",
        "    return text"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rm1pJAVH4Xno"
      },
      "outputs": [],
      "source": [
        "def little_process(text):\n",
        "    \n",
        "    text = str(text)\n",
        "    \n",
        "    text = remove_links(text)\n",
        "\n",
        "    text = remove_double_spaces(text)\n",
        "\n",
        "\n",
        "    return text"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "g_r01Hu5Vv1Z"
      },
      "outputs": [],
      "source": [
        "def double_extra_cleaning(text):\n",
        "  text = str(text)\n",
        "\n",
        "  text = text.lower()\n",
        "\n",
        "  text = remove_links(text)\n",
        "\n",
        "  text = remove_special_characters(text)\n",
        "\n",
        "  text = remove_double_spaces(text)\n",
        "\n",
        "  text = remove_stopwords(text)\n",
        "\n",
        "  return text"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4e7u_8iLVdPU"
      },
      "outputs": [],
      "source": [
        "def extra_cleaning(text):\n",
        "    text = str(text)\n",
        "\n",
        "    text = text.lower()\n",
        "    \n",
        "    text = remove_links(text)\n",
        "    \n",
        "    text = remove_special_characters(text)\n",
        "\n",
        "    text = remove_double_spaces(text)\n",
        "\n",
        "    return text"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nUtTEr0PImzO"
      },
      "outputs": [],
      "source": [
        "def special_cleaning(text):\n",
        "    text = text.lower()\n",
        "    \n",
        "    text = remove_links(text)\n",
        "    \n",
        "    text = expand_contractions(text)\n",
        "    \n",
        "    text = accented_characters_removal(text)\n",
        "    \n",
        "    text = remove_special_characters(text)\n",
        "\n",
        "    return text"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "Bk9vdaobL8yo"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}